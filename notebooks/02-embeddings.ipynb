{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adab3cc7",
   "metadata": {},
   "source": [
    "# 2: Analyzing and Visualizing Text Embeddings for Semantic Similarity using Amazon Bedrock Embeddings\n",
    "\n",
    "- SageMaker Notebook Kernel: `conda_python3`\n",
    "- SageMaker Notebook Instance Type: ml.m5d.large | ml.t3.large\n",
    "\n",
    "In this notebook, you'll explore semantic text similarity by generating, analyzing, and visualizing embeddings for a collection of sentences. It leverages [Amazon Bedrock](https://aws.amazon.com/bedrock/) embeddings for generating high-dimensional vector representations of textual data. For visualization, the notebook employs t-Distributed Stochastic Neighbor Embedding (t-SNE), a dimensionality reduction technique, to plot embeddings in a 2D space. It further investigates similarity metrics by computing and visualizing cosine similarity between texts. \n",
    "\n",
    "## Runtime \n",
    "\n",
    "This notebook takes approximately 10 minutes to run.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Prerequisites](#prerequisites)\n",
    "1. [Setup](#setup)\n",
    "1. [Embeddings](#embeddings)\n",
    "1. [Visualize similarity with t-SNE](#visualize-similarity-with-t-sne)\n",
    "1. [Compute the cosine similarity between texts](#compute-the-cosine-similarity-between-texts)\n",
    "1. [Test different texts](#test-different-texts)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "`amazon.titan-embed-text-v1` enabled in the Amazon Bedrock console in `us-west-2`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a98a83",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing and importing the required packages for this notebook. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Note:</b> Verify that the notebook kernel is `conda_python3`. Also, if you run into an issue where a module can't be imported after installation, restart the notebook kernel, then rerun the import notebook cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7f67d-fb1e-4dfd-9082-6ca50626b756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker --quiet\n",
    "%pip install pandas --quiet\n",
    "%pip install numexpr --quiet\n",
    "%pip install scikit-learn --quiet\n",
    "%pip install matplotlib --quiet\n",
    "%pip install seaborn --quiet\n",
    "%pip install matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26418e-f009-4af8-936b-e1081ba85f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "from IPython.display import display\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.manifold import TSNE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01f971",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Next, we will initialize the Amazon Bedrock boto3 client. The embeddings model we will use for is `amazon.titan-embed-text-v1`. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54635a5e-fd7b-4c74-bd76-780d29a3f4ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "embeddings_model_id = \"amazon.titan-embed-text-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7af2cd",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Create a helper method called `get_embeddings`, which will properly format our request to the embeddings model and will handle extracting the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b4aae-3fa8-4aaf-91fc-756cf2122bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    input_body = {\"inputText\": text}\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=json.dumps(input_body),\n",
    "        modelId=embeddings_model_id,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body.get(\"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f02fbd",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Get the embeddings for the question `What is Amazon Bedrock` and take a look at the output.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c345994-485a-4729-ad7a-336a8ba3659b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = get_embedding(\"What is Amazon Bedrock?\")\n",
    "\n",
    "embeddings = np.array(response)\n",
    "embeddings_dimensions = len(embeddings)\n",
    "\n",
    "display(embeddings)\n",
    "print(f\"Vector Dimensions: {embeddings_dimensions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb8b21",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Text embeddings are numerical representations of text data that capture semantic and contextual information about words, phrases, or entire documents. The sentence `What is Amazon Bedrock?` was transformed into a vector, which is an ordered sequence of numbers, similar to a list or an array. The number of values in a text embedding is known as its dimensions. The transformer based Amazon Bedrock embeddings model returns a vector with a fixed size 1536 dimensions. This dense vector numerically represents the semantic and contextual relationships of the input text. \n",
    "\n",
    "Now, let's take a look at how we can use embeddings to determine similarity between different texts.\n",
    "\n",
    "First, we will create a helper function to create embeddings for an array of texts and store it in [Pandas](https://pandas.pydata.org/) [Dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). Pandas is a library that provides data structures for efficiently storing large or complex data sets and data analysis tools and a DataFrame is a tabular data structure that makes it easy to handle and display data in our notebook.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_df(texts):\n",
    "    df = pd.DataFrame(texts, columns=[\"text\"])\n",
    "    df[\"embedding\"] = df[\"text\"].apply(get_embedding)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72df1c7",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Next, generate embeddings for the texts below. Take a look at the lines of text and think about which ones you think are similar and which ones aren't.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"How do I deploy a SageMaker endpoint?\",\n",
    "    \"I need instructions to deploy a ML model endpoint\",\n",
    "    \"RDS, DynamoDB, and Neptune\",\n",
    "    \"Relational database, key-value database, and graph database\",\n",
    "    \"Large language models on Amazon Bedrock\",\n",
    "]\n",
    "df = get_embeddings_df(texts)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4b4e5",
   "metadata": {},
   "source": [
    "## Visualize similarity with t-SNE\n",
    "\n",
    "Understanding textual relationships in their raw form is easy for us humans, but comprehending these relationships when they are embedded in a 1536-dimensional vector space is not as easy. This is where [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) is helpful. t-SNE is a dimensionality reduction technique commonly used in machine learning and data visualization. It is particularly useful for visualizing high-dimensional data in a lower-dimensional space while preserving the pairwise similarities between data points. \n",
    "\n",
    "Let's visualize the sentence embeddings using t-SNE from scikit-learn. Since it's a projection to 2D space. Notice how related items are closer together and unrelated items are further away?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tsne_plot(df):\n",
    "\n",
    "    # Convert the list of embeddings to a NumPy array\n",
    "    embeddings = np.array(df[\"embedding\"].tolist())\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\", random_state=4, perplexity=3)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=\"blue\", alpha=0.6, label=\"Embeddings\")\n",
    "\n",
    "    # Annotate each point with the corresponding text\n",
    "    for i, txt in enumerate(df[\"text\"]):\n",
    "        plt.annotate(\n",
    "            txt, (embeddings_2d[i, 0], embeddings_2d[i, 1]), textcoords=\"offset points\", xytext=(0, 5), ha=\"center\"\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.title(\"2D Visualization of Text Embeddings using t-SNE\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_tsne_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86ede7",
   "metadata": {},
   "source": [
    "## Compute the cosine similarity between texts\n",
    "\n",
    "Luckily, computers are much more capable than we are in higher dimensions. There are a few methods of determining similarity between vectors. One way is to measure the euclidean distance between the vectors which calculates the straight-line distance between two points in a vector space. It considers both the direction and the magnitude of the vectors. Another method is measuring the cosine of the angle between two vectors effectively determining whether they are pointing in roughly the same direction, irrespective of their magnitude. Euclidean distance is more suitable when magnitude is a significant factor, while cosine similarity is often used in text analysis and other domains where the direction of the data vectors is more important than their magnitude, so we will use cosine similarity. The formula for the cosine similarity between two vectors is is the dot product of the vector divided by the dot product of the norms of vectors x and y respectively.\n",
    "\n",
    "$$\\text{d}(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\|_2 \\times \\|\\mathbf{y}\\|_2}$$\n",
    "\n",
    "\n",
    "Let's compute the cosine similarity between the vectors and plot the values on a heat map. On the heatmap, you'll observe cosine values ranging from -1, meaning exactly opposite, to 1 meaning exactly similar. In vector terms, -1 represents vectors pointing in opposite directions, 0 represents vectors that are orthogonal to each other, and 1 represents vectors pointing in the same direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_similarity_matrix(df, similarity_fn=cosine_similarity):\n",
    "    # Convert the list of embeddings into a NumPy array\n",
    "    embeddings_matrix = np.array(df[\"embedding\"].tolist())\n",
    "\n",
    "    # Calculate the cosine similarity matrix\n",
    "    similarity_matrix = similarity_fn(embeddings_matrix)\n",
    "\n",
    "    # Create a DataFrame for the cosine similarity matrix with row and column headers\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=df.index, columns=df.index)\n",
    "\n",
    "    similarity_df_rounded = similarity_df.round(2)\n",
    "\n",
    "    # print the index and text column of the df\n",
    "    display(df[[\"text\"]])\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    cmap = sns.diverging_palette(10, 240, n=9, as_cmap=True)\n",
    "\n",
    "    # Generate the heatmap with the new color map\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    g = sns.heatmap(similarity_df_rounded, annot=True, cmap=cmap, cbar=True, linewidths=0.5, center=0)\n",
    "    g.xaxis.tick_top()\n",
    "\n",
    "    plt.title(f\"{similarity_fn.__name__} Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_similarity_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e93ec",
   "metadata": {},
   "source": [
    "## Test different texts\n",
    "\n",
    "If you were having trouble seeing the color differences in the previous example, try the next example which has greater separation between the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcaf7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "\t\"Can you please tell me how to get to the bakery?\",\n",
    "\t\"I need directions to the bread shop\",\n",
    "\t\"Cats, dogs, and mice\",\n",
    "\t\"Felines, canines, and rodents\",\n",
    "\t\"Four score and seven years ago\"\n",
    "]\n",
    "\n",
    "df2 = get_embeddings_df(texts)\n",
    "show_tsne_plot(df2)\n",
    "show_similarity_matrix(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dddb03",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now, replace the text in the array below with your own sentences and see how they compare. Some things to try are different lengths of text even text in other languages.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74774a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "\t\"text\",\n",
    "\t\"text\",\n",
    "\t\"text\",\n",
    "\t\"text\",\n",
    "]\n",
    "\n",
    "df3 = get_embeddings_df(texts)\n",
    "show_tsne_plot(df3)\n",
    "show_similarity_matrix(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b4e3c3",
   "metadata": {},
   "source": [
    "## Notebook complete\n",
    "\n",
    "Embeddings serve as the foundation of semantic search engines and advanced question answering (QA) systems. In the next notebook, we will look how to extract and load data into to vector store to use for similarity search.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
