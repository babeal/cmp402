{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1741d5-a595-43ee-b2cd-d47323af5333",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0: Download and Compile Llama 2 7b Chat weights for AWS Neuron (HuggingFace)\n",
    "\n",
    "- Neuronx 2.15\n",
    "- SageMaker Notebook Kernel: `conda_python3`\n",
    "- SageMaker Notebook Instance Type: ml.m5d.large | ml.t3.large\n",
    "\n",
    "In this notebook, we will prepare the Llama 2 Chat instruction tuned large language model from Facebook to run on [AWS Tranium (Trn1)](https://aws.amazon.com/ec2/instance-types/trn1/) accelerators. We will create a python script to download the model from Hugging Face, the use the  the `transformers-neuronx` package to transform and compile the model weights for Neuron. Then we will create and submit a SageMaker training job to execute the script on Tranium (trn1) then upload the model artifacts to S3.\n",
    "\n",
    "Elastic Compute Cloud (EC2) Trn1 instances, powered by AWS Trainium accelerators, are purpose built for high-performance deep learning (DL) training of generative AI models, including large language models (LLMs) and latent diffusion models. Trn1 instances offer up to 50% cost-to-train savings over other comparable Amazon EC2 instances. You can use Trn1 instances to train 100B+ parameter DL and generative AI models across a broad set of applications, such as text summarization, code generation, question answering, image and video generation, recommendation, and fraud detection. [AWS Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/). This notebook uses the **2.14.1** version of Neuron SDK.\n",
    "\n",
    "## Runtime \n",
    "\n",
    "This notebook takes approximately 30 minutes to run (after prerequisites have been met)\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Prerequisites](#prerequisites)\n",
    "1. [Setup](#setup)\n",
    "1. [Prepare and execute the training job](#prepare-and-execute-the-training-job)\n",
    "1. [Save the model location](#save-the-model-location)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You will need to complete the following steps to get permission to download LLama2 pre-trained weights from Meta. \n",
    "\n",
    "### Create Hugging Face account\n",
    "\n",
    "Go to (https://huggingface.co/join) and create a Hugging Face account if you don't have one. Log into HF hub after that.\n",
    "\n",
    "### Step 2 - Create an Access token\n",
    "\n",
    "Follow the instructions from (https://huggingface.co/docs/hub/security-tokens) and create a new Access token. Copy the token.\n",
    "\n",
    "### Step 3 - Meta approval to download weights\n",
    "\n",
    "Follow the instructions from (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) to get an approval from Meta for you to download and use the weights. It can take some time. After approved you'll see a message like: Gated model You have been granted access to this model at the top of the same page. Now you're ready to download and compile your model to Inferentia2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd95005",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing and importing the required packages for this notebook. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Note:</b> Verify that the notebook kernel is `conda_python3`. Also, if you run into an issue where a module can't be imported after installation, restart the notebook kernel, then rerun the import notebook cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac49ff3-99b5-4ece-b8ea-d36d775b50fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker --quiet\n",
    "%pip install python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import sagemaker\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "from ipywidgets import widgets\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223de0a7",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Next, we will initialize the SageMaker session and create a working directory.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248a658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker_session.get_caller_identity_arn()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# Create the working directory\n",
    "os.makedirs(\"build/train\", exist_ok=True)\n",
    "\n",
    "# Print the session detail\n",
    "print(f\"Sagemaker version: {sagemaker.__version__}\")\n",
    "print(f\"Sagemaker role arn: {role}\")\n",
    "print(f\"Sagemaker bucket: {bucket}\")\n",
    "print(f\"Sagemaker region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3258c4",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Downloading the Llama 2 model weights from Hugging Face requires a user access token. After running the next cell, a input text box will appear below the cell where you should enter the token. Once entered, select the cell below the text box and run it. This will write the token to a .env file on the notebook server and clear the input text box to prevent the token from being saved with the notebook.\n",
    "\n",
    "If you have already done this step in a previous execution you don't need to enter the token again. If you want to change the token value or if it was input incorrectly you can set the `force_overwrite` variable to True and it will replace the value in the file.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d758e-1f55-4a5e-a685-8fad0e72e889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = widgets.Password(placeholder=\"Enter Token\", description=\"Hugging Face Token:\", disabled=False)\n",
    "display(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a2518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "force_overwrite = False\n",
    "env_file = \".env\"\n",
    "\n",
    "# write the access token to the .env file\n",
    "if not os.path.exists(env_file) or force_overwrite:\n",
    "    print(\"Creating environment file\")\n",
    "    with open(env_file, \"w\") as file:\n",
    "        file.write(f\"HF_TOKEN={input_text.value}\")\n",
    "else:\n",
    "    print(\"File already exists\")\n",
    "# clear the input value\n",
    "input_text.value = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be26a23",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Load the environment file and test that the token has a value.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4efa7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "assert (\n",
    "    os.getenv(\"HF_TOKEN\") != \"\"\n",
    "), \"Go to your HF account and get an access token. Set HF_TOKEN in the .env file with the token value.\"\n",
    "os.makedirs(\"build/train\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac096403",
   "metadata": {},
   "source": [
    "## Prepare and execute the training job\n",
    "\n",
    "Next, we will create files needed by the SageMaker training job to prepare the model for Inferentia 2. \n",
    "\n",
    "- `requirements.txt` - Install packages needed by the prepare_llama2.py script\n",
    "- `prepare_llama2.py` - The training job script to download the model from hugging face\n",
    "\n",
    "Read through the `prepare_llama2.py` script to understand what it's doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0c3f2-7885-4dd8-9a7d-1f0a011e6127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build/train/requirements.txt\n",
    "\n",
    "-i https://pip.repos.neuron.amazonaws.com\n",
    "torchserve==0.9.0\n",
    "sentencepiece==0.1.99\n",
    "transformers==4.34.1\n",
    "neuronx-cc==2.11.0.34\n",
    "torch-neuronx==1.13.1.1.12.0\n",
    "transformers-neuronx==0.8.268\n",
    "torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48721f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build/train/prepare_llama2.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import argparse\n",
    "import traceback\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "from transformers_neuronx.llama.model import LlamaForSampling\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"meta-llama/Llama-2-7b-chat-hf\")    \n",
    "    parser.add_argument(\"--hf_access_token\", type=str, default=os.environ[\"HF_TOKEN\"])\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--tp_degree\", type=int, default=2)\n",
    "    parser.add_argument(\"--n_positions\", type=int, default=1024)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"bf16\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Set Neuron environment variables\n",
    "    os.environ[\"NEURON_COMPILE_CACHE_URL\"]=os.path.join(args.model_dir, \"neuron_cache\")\n",
    "    # Specifies the number of NeuronCores to be used at runtime and it should match the tensor parallelism (TP) degree specified for the model\n",
    "    os.environ[\"NEURON_RT_NUM_CORES\"] = str(args.tp_degree)\n",
    "    # Enables compiler optimization on decoder-only LLM models.\n",
    "    # -O1 - not optimized for performance\n",
    "    # -O2 - default settings\n",
    "    # -O3 - best performance\n",
    "    os.environ[\"NEURON_CC_FLAGS\"] = \"-O3\"\n",
    "\n",
    "    # log into HuggingFace\n",
    "    login(args.hf_access_token)\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    t=time.time()\n",
    "\n",
    "    # Loads model weights from HuggingFace\n",
    "    model = LlamaForCausalLM.from_pretrained(args.model_id) \n",
    "    print(f\"Elapsed: {time.time()-t}s\")\n",
    "\n",
    "    print(\"Splitting and saving...\")\n",
    "    t=time.time()\n",
    "\n",
    "    save_pretrained_split(model, os.path.join(args.model_dir, \"llama2-split\")) \n",
    "    \n",
    "    print(f\"Elapsed: {time.time()-t}s, Done\")    \n",
    "\n",
    "    print(\"Saving tokenizer...\")\n",
    "    t=time.time()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id) # Loads tokenizer from HuggingFace\n",
    "    tokenizer.save_pretrained(args.model_dir) # Saves tokenizer to output model directory\n",
    "\n",
    "    print(f\"Elapsed: {time.time()-t}s, Done\")\n",
    "\n",
    "    kwargs = {\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"amp\": args.dtype,\n",
    "        \"tp_degree\": args.tp_degree,\n",
    "        \"n_positions\": args.n_positions,\n",
    "        \"unroll\": None\n",
    "    }\n",
    "\n",
    "    print(\"Compiling model...\")\n",
    "    t=time.time()\n",
    "\n",
    "    model = LlamaForSampling.from_pretrained(os.path.join(args.model_dir, \"llama2-split\"), **kwargs)\n",
    "    model.to_neuron()\n",
    "    neuron_model.save(os.path.join(model_dir, 'neuron_artifacts'))\n",
    "\n",
    "    print(f\"Compilation time: {time.time()-t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fb77e-76f7-43ec-b9bd-8b1d01db0946",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Set the training job parameters\n",
    "\n",
    "- `tp_degree` - Sets the number of neuron cores to be used during compilation; this needs to match the target. Since we will be running this model on an Inf2.xlarge instance type which has 1 accelerator and there are 2 cores per accelerator, tp_degree should be set to 2.\n",
    "- `batch_size` - The batch size number\n",
    "- `sentence_length` - The maximum sequence length that this model might ever be used with. For Llama 2 the max token length is 4096.\n",
    "- `instance_type` - the SageMaker instance on which the training job will execute.\n",
    "- `image_uri` - the ECR container image url. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cebcd92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tp_degree = 2  # set to the number of neuron cores available (2 * accelerators)\n",
    "dtype = \"f16\"\n",
    "batch_size = 1\n",
    "sentence_length = 4096\n",
    "instance_type = \"ml.trn1.2xlarge\"\n",
    "image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.14.1-ubuntu20.04\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445c925",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Create the SageMaker PyTorch estimator and run the job.\n",
    "\n",
    "This step can take up to **25 minutes**. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691f3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"prepare_llama2.py\",  # Specify your train script\n",
    "    source_dir=\"build/train\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "    image_uri=image_uri,\n",
    "    volume_size=128,\n",
    "    environment={\n",
    "        \"HF_TOKEN\": os.getenv(\"HF_TOKEN\"),\n",
    "        \"FI_EFA_FORK_SAFE\": \"1\",  # https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/nrt-troubleshoot.html\n",
    "    },\n",
    "    hyperparameters={\"model_id\": \"meta-llama/Llama-2-7b-chat-hf\", \"tp_degree\": tp_degree, \"n_positions\": sentence_length},\n",
    ")\n",
    "estimator.framework_version = \"1.13.1\"  # workround when using image_uri\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b44e6ed-9300-498e-859f-3acaa767e3fb",
   "metadata": {},
   "source": [
    "## Save the model location\n",
    "\n",
    "Write the training job model information to disk to be reused in `03-deploy-model.ipynb` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e218e94-598b-47dd-85ee-db97d0d639ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"model_data.json\", \"w\") as file:\n",
    "    file.write(json.dumps(estimator.model_data))\n",
    "\n",
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c8dc44-c110-44fb-a3d5-7fb39338df7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook complete\n",
    "\n",
    "You've finished preparing the model for Inferentia. Please move to the next workbook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
