{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7023fd94",
   "metadata": {},
   "source": [
    "# 1: Deploy Llama 2 7b Chat HF to Inferentia2 on SageMaker\n",
    "\n",
    "- Neuronx 2.15\n",
    "- SageMaker Notebook Kernel: `conda_python3`\n",
    "- SageMaker Notebook Instance Type: ml.m5d.large | ml.t3.large\n",
    "\n",
    "In this notebook, you will use the SageMaker Python SDK to deploy a Llama 2 Chat 7b instruction tuned large language model on an endpoint with [AWS Inferentia 2 (Inf2)](https://aws.amazon.com/ec2/instance-types/inf2/) accelerators. You will create the files needed to run inference on TorchServe within a `pytorch-inference-neuronx` container. Then you will deploy and test the endpoint.\n",
    "\n",
    "AWS Inferentia 2 instances are purpose built for deep learning (DL) inference. They deliver high performance at the lowest cost in Amazon EC2 and SageMaker for generative artificial intelligence (AI) models, including large language models (LLMs). The [AWS Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/) helps developers deploy models on the AWS Inferentia accelerators. It integrates natively with frameworks, such as PyTorch and TensorFlow, so you can continue using your existing workflows and application code and run on Inf2 instances.\n",
    "\n",
    "## Runtime \n",
    "\n",
    "This notebook takes approximately 20 minutes to run.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Prerequisites](#prerequisites)\n",
    "1. [Setup](#setup)\n",
    "1. [Build and push inference container](#build-and-push-inference-container)\n",
    "1. [Create the TorchServe inference files](#create-the-torchserve-inference-files)\n",
    "1. [Update the model archive](#update-the-model-archive)\n",
    "1. [Create and deploy model endpoint](#create-and-deploy-model-endpoint)\n",
    "1. [Test the endpoint](#test-the-endpoint)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Neuron 2.15 (tp=2) compiled LLama 2 Chat model weights (created by notebook 00 or s3 uri provided by workshop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f002a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing and importing the required packages for this notebook. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Note:</b> Verify that the notebook kernel is `conda_python3`. Also, if you run into an issue where a module can't be imported after installation, restart the notebook kernel, then rerun the import notebook cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c0b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker --quiet\n",
    "%pip install torch-model-archiver --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6add2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "from sagemaker import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273f515",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Next, we will initialize the SageMaker session and create a working directory.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30d8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "smr = sagemaker_session.sagemaker_runtime_client\n",
    "role = sagemaker_session.get_caller_identity_arn()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "account = sagemaker_session.account_id()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "os.makedirs(\"build/inference\", exist_ok=True)\n",
    "os.makedirs(\"build/inference/container\", exist_ok=True)\n",
    "\n",
    "print(f\"Sagemaker version: {sagemaker.__version__}\")\n",
    "print(f\"Sagemaker role arn: {role}\")\n",
    "print(f\"Sagemaker bucket: {bucket}\")\n",
    "print(f\"Sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2b0d6",
   "metadata": {},
   "source": [
    "## Build and push inference container\n",
    "\n",
    "The 2.15 version of [Neuron inference container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#neuron-containers) hasn't been released yet. Let's upgrade the 2.14.1 version with the latest Neuron, transformers, and torchserve packages. \n",
    "\n",
    "- `Dockerfile` - Custom container definition with `pytorch-inference-neuronx` as the base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f61de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build/inference/container/Dockerfile\n",
    "\n",
    "FROM \"763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.14.1-ubuntu20.04\"\n",
    "\n",
    "RUN pip install --upgrade \\ \n",
    "    torchserve==0.9.0 \\\n",
    "    neuronx-cc==2.11.0.34 \\\n",
    "    sentencepiece==0.1.99 \\\n",
    "    torch-neuronx==1.13.1.1.12.0 \\\n",
    "    transformers==4.34.1 \\\n",
    "    transformers-neuronx==0.8.268 \\\n",
    "    torchvision \\\n",
    "    --extra-index-url=https://pip.repos.neuron.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b7c88",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Pull the base `pytorch-inference-neuronx` container, run docker build, then upload the custom container to AWS ECR. This cell can take **10** minutes to run and doesn't produce any output, so please be patient.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_build_out, docker_build_err = \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ff3c6-a918-4bc0-8645-41b38ec84b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh --out docker_build_out --err docker_build_err\n",
    "\n",
    "# The name of ECR repository to create and push to\n",
    "repository_name=\"neuronx-inference\"\n",
    "\n",
    "cd build/inference/container\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "source_image_name=$(awk -F ' ' '/^FROM/ { gsub(/\"/, \"\", $2); print $2 }' Dockerfile)\n",
    "\n",
    "target_image_name=\"${account}.dkr.ecr.${region}.amazonaws.com/${repository_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${repository_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${repository_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# if the image doesn't exist in ECR, create it.\n",
    "if ! aws ecr list-images --repository-name \"${repository_name}\" | jq -e '.imageIds | length > 0' >/dev/null; then\n",
    "    aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${source_image_name}\n",
    "\n",
    "    # Get the login command from ECR and execute it directly\n",
    "    aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${target_image_name}\n",
    "\n",
    "    # Build the Docker image locally with the image name and then push it to ECR\n",
    "    docker build -q -t ${repository_name} --build-arg region=${region} .\n",
    "    docker tag ${repository_name} ${target_image_name}\n",
    "\n",
    "    docker push ${target_image_name}\n",
    "fi\n",
    "\n",
    "# print the name of the container so we can get it in the next cell\n",
    "echo ${target_image_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea8251",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now, let's check the output of the previous command, and if successful, store the image uri in a variable for use when we deploy the endpoint.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27064a97-3c4e-4d46-a02d-34277868a8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's check the docker build output for errors. If successful, print the docker image uri.\n",
    "if \"Error response from daemon\" in str(docker_build_err):\n",
    "    print(docker_build_err)\n",
    "    raise SystemExit(\"\\n\\n!!There was an error with the container build!!\")\n",
    "else:\n",
    "    image_uri = str(docker_build_out).strip().split(\"\\n\")[-1]\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338c437a",
   "metadata": {},
   "source": [
    "## Create the TorchServe inference files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438ec83-ffc9-4dca-87f1-5cf9f021c33f",
   "metadata": {},
   "source": [
    "\n",
    "Create the `inference.py` file. This file contains the core inference logic and input and output handling.  \n",
    "\n",
    "Notable methods:\n",
    "\n",
    "- `initialize` - Handles the load and initialization of the model. If you have used the HuggingFace transformers library before, then this code may look familiar. The line `self.model.to_neuron()` handles loading the model onto the accelerators. \n",
    "- `preprocess` - Deserializes the request and passes the input prompts into the tokenizer. \n",
    "- `inference` - Invokes the model with the request parameters and tokenized input. Supports both streaming and non-streaming response modes.\n",
    "- `postprocess` - Passes the tokens generated by the model to the tokenizer for decoding back to text before it's returned to the client.\n",
    "\n",
    "Read through the inference.py file to understand what it's doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c871ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build/inference/inference.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from abc import ABC\n",
    "from threading import Thread\n",
    "\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoConfig, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, MaxLengthCriteria\n",
    "from transformers_neuronx.generation_utils import HuggingFaceGenerationModelAdapter\n",
    "from transformers_neuronx.llama.model import LlamaForSampling\n",
    "\n",
    "from ts.handler_utils.hf_batch_streamer import TextIteratorStreamerBatch\n",
    "from ts.handler_utils.micro_batching import MicroBatching\n",
    "from ts.protocol.otf_message_handler import send_intermediate_predict_response\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "from ts.utils.util import PredictionException\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LLMHandler(BaseHandler, ABC):\n",
    "    \"\"\"\n",
    "    Transformers handler class for text completion streaming on Inferentia2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.initialized = False\n",
    "        self.max_length = None\n",
    "        self.tokenizer = None\n",
    "        self.output_streamer = None\n",
    "        # enable micro batching\n",
    "        self.handle = MicroBatching(self)\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        self.manifest = ctx.manifest\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        model_checkpoint_dir = ctx.model_yaml_config.get(\"handler\", {}).get(\n",
    "            \"model_checkpoint_dir\", \"\"\n",
    "        )\n",
    "        model_checkpoint_path = f\"{model_dir}/{model_checkpoint_dir}\"\n",
    "\n",
    "        os.environ[\"NEURON_COMPILE_CACHE_URL\"] = f\"{model_dir}/neuron_cache\"\n",
    "\n",
    "        # -O1 - not optimized for performance\n",
    "        # -O2 - default settings\n",
    "        # -O3 - best performance\n",
    "        os.environ[\"NEURON_CC_FLAGS\"] = \"-O3\" \n",
    "\n",
    "        # micro batching initialization\n",
    "        micro_batching_parallelism = ctx.model_yaml_config.get(\n",
    "            \"micro_batching\", {}\n",
    "        ).get(\"parallelism\", None)\n",
    "        if micro_batching_parallelism:\n",
    "            logger.info(\n",
    "                f\"Setting micro batching parallelism from model_config_yaml: {micro_batching_parallelism}\"\n",
    "            )\n",
    "            self.handle.parallelism = micro_batching_parallelism\n",
    "\n",
    "        micro_batch_size = ctx.model_yaml_config.get(\"micro_batching\", {}).get(\n",
    "            \"micro_batch_size\", 1\n",
    "        )\n",
    "        logger.info(f\"Setting micro batching size: {micro_batch_size}\")\n",
    "        self.handle.micro_batch_size = micro_batch_size\n",
    "\n",
    "        # settings for model compilation and loading\n",
    "        amp = ctx.model_yaml_config.get(\"handler\", {}).get(\"amp\", \"f32\")\n",
    "        tp_degree = ctx.model_yaml_config.get(\"handler\", {}).get(\"tp_degree\", 6)\n",
    "        self.max_length = ctx.model_yaml_config.get(\"handler\", {}).get(\"max_length\", 50)\n",
    "\n",
    "        # allocate \"tp_degree\" number of neuron cores to the worker process\n",
    "        os.environ[\"NEURON_RT_NUM_CORES\"] = str(tp_degree)\n",
    "        try:\n",
    "            num_neuron_cores_available = (\n",
    "                torch_neuronx.xla_impl.data_parallel.device_count()\n",
    "            )\n",
    "            assert num_neuron_cores_available >= int(tp_degree)\n",
    "        except (RuntimeError, AssertionError) as error:\n",
    "            logger.error(\n",
    "                \"Required number of neuron cores for tp_degree \"\n",
    "                + str(tp_degree)\n",
    "                + \" are not available: \"\n",
    "                + str(error)\n",
    "            )\n",
    "\n",
    "            raise error\n",
    "\n",
    "        logger.info(\"LOADING TOKENIZER\")\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        logger.info(\"LOADING MODEL\") \n",
    "        self.model = LlamaForSampling.from_pretrained(\n",
    "            model_checkpoint_path,\n",
    "            batch_size=self.handle.micro_batch_size,\n",
    "            amp=amp,\n",
    "            tp_degree=tp_degree,\n",
    "        )\n",
    "\n",
    "        # Load compiled artifacts if they exist\n",
    "        neuron_artifacts = os.path.join(model_dir, \"neuron_artifacts\")\n",
    "        if os.path.isdir(neuron_artifacts):\n",
    "            logger.info(\"LOADING COMPILED ARTIFACTS FROM CACHE\")\n",
    "            self.model.load(neuron_artifacts)\n",
    "        else:\n",
    "            logger.debug(\"COMPILING MODEL FOR NEURON\")\n",
    "\n",
    "        self.model.to_neuron()\n",
    "        logger.info(\"Model has been successfully compiled\")\n",
    "\n",
    "        logger.debug(\"LOADING CONFIG\")\n",
    "        \n",
    "        model_config = AutoConfig.from_pretrained(model_checkpoint_path)\n",
    "        self.model = HuggingFaceGenerationModelAdapter(model_config, self.model)\n",
    "        \n",
    "        # https://github.com/pytorch/serve/blob/447f3ef2b6df2ea9171c219b6f3bd51d2f0adbc5/ts/handler_utils/hf_batch_streamer.py#L7\n",
    "        self.output_streamer = TextIteratorStreamerBatch(\n",
    "            self.tokenizer,\n",
    "            batch_size=self.handle.micro_batch_size,\n",
    "            skip_prompt=True,\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        logger.debug(\"PREPROCESS\")\n",
    "        input_text = []\n",
    "        requests = []\n",
    "        logger.info(f\"Preprocessing {len(data)} requests\")\n",
    "        for req in data:\n",
    "            body = req.get(\"body\")\n",
    "            request = json.loads(body)\n",
    "            logger.debug(request)\n",
    "            prompt = request[\"prompt\"]\n",
    "            input_text.append(prompt.strip())\n",
    "            requests.append(request)\n",
    "\n",
    "        # Ensure the compiled model can handle the input received\n",
    "        if len(input_text) > self.handle.micro_batch_size:\n",
    "            raise ValueError(\n",
    "                f\"Model is compiled for batch size {self.handle.micro_batch_size} but received input of size {len(input_text)}\"\n",
    "            )\n",
    "\n",
    "        # Pad input to match compiled model batch size\n",
    "        input_text.extend([\"\"] * (self.handle.micro_batch_size - len(input_text)))\n",
    "        \n",
    "        return self.tokenizer(\n",
    "            input_text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,\n",
    "            add_special_tokens=False # to turn off adding of <s> at the begging of the input\n",
    "        ), requests\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, data, *args, **kwargs):\n",
    "        logger.debug(\"INFERENCE\")\n",
    "        tokenized_input, requests = data\n",
    "        # use the first request to set the hyper parameters\n",
    "        request = requests[0]\n",
    "        \n",
    "        calling_method = self.context.get_request_header(0, \"X-Amzn-SageMaker-Forwarded-API\")\n",
    "        if calling_method == \"InvokeEndpointWithResponseStream\":\n",
    "            streaming = True\n",
    "        else:\n",
    "            streaming = False\n",
    "\n",
    "        parameters = dict(\n",
    "            max_new_tokens=request.get(\"max_tokens_to_sample\", self.max_length),\n",
    "            temperature=request.get(\"temperature\", 0.6),\n",
    "            top_p=request.get(\"top_p\", 0.9),\n",
    "            top_k=request.get(\"top_k\", 50),\n",
    "            do_sample=True,\n",
    "        )\n",
    "        logger.info(parameters)\n",
    "\n",
    "        self.model.reset_generation()\n",
    "\n",
    "        if streaming:\n",
    "            # see https://huggingface.co/docs/transformers/main_classes/text_generation for options\n",
    "            logger.debug(\"Using hf model generate streaming method\")\n",
    "            generation_kwargs = dict(\n",
    "                tokenized_input,\n",
    "                streamer=self.output_streamer,  \n",
    "                **parameters\n",
    "            )   \n",
    "            \n",
    "            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
    "            thread.start()\n",
    "    \n",
    "            micro_batch_idx = self.handle.get_micro_batch_idx()\n",
    "            micro_batch_req_id_map = self.get_micro_batch_req_id_map(micro_batch_idx)\n",
    "            for new_texts in self.output_streamer:\n",
    "                formatted_responses = [ f\"{json.dumps({'outputs': [new_text]})}\\n\" for new_text in new_texts[: len(micro_batch_req_id_map)]]\n",
    "                send_intermediate_predict_response(\n",
    "                    formatted_responses,\n",
    "                    micro_batch_req_id_map,\n",
    "                    \"Intermediate Prediction success\",\n",
    "                    200,\n",
    "                    self.context,\n",
    "                )\n",
    "    \n",
    "            thread.join()\n",
    "    \n",
    "            return [\"\"] * len(micro_batch_req_id_map)\n",
    "        else:\n",
    "\n",
    "            generation_kwargs = dict(\n",
    "                tokenized_input,\n",
    "                **parameters\n",
    "            )\n",
    "    \n",
    "            generated_sequences = self.model.generate(**generation_kwargs)\n",
    "    \n",
    "            # remove input tokens from response\n",
    "            cleaned_generated_sequences = [gs[len(tokenized_input[\"input_ids\"][idx]):] for idx, gs in enumerate(generated_sequences)]\n",
    "           \n",
    "            return cleaned_generated_sequences\n",
    "\n",
    "    def postprocess(self, data):\n",
    "        logger.debug(\"IN POST PROCESS\")\n",
    "        decoded_sequences = self.tokenizer.batch_decode(data, skip_special_tokens=True)\n",
    "        \n",
    "        # Post process gets called for both streaming and non-streaming requests\n",
    "        # The streaming processor is looking for a \\n as an indication that all bytes for\n",
    "        # that message have been received, which is why it's added to the end of the response\n",
    "        responses = [f\"{json.dumps({'outputs': [seq] })}\\n\"  for seq in decoded_sequences]\n",
    "        \n",
    "        return responses   \n",
    "    \n",
    "    def handle(self, data, context):\n",
    "        try:\n",
    "            super().handle(data, context)\n",
    "        except Exception as e:\n",
    "            raise PredictionException(\"Unable to process request. \" + str(e))\n",
    "\n",
    "\n",
    "    def get_micro_batch_req_id_map(self, micro_batch_idx: int):\n",
    "        start_idx = micro_batch_idx * self.handle.micro_batch_size\n",
    "        micro_batch_req_id_map = {\n",
    "            index: self.context.request_ids[batch_index]\n",
    "            for index, batch_index in enumerate(\n",
    "                range(start_idx, start_idx + self.handle.micro_batch_size)\n",
    "            )\n",
    "            if batch_index in self.context.request_ids\n",
    "        }\n",
    "\n",
    "        return micro_batch_req_id_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b8030b-95c7-4396-b844-085ac5785d84",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Create the [TorchServe](https://pytorch.org/serve/large_model_inference.html) `model-config.yaml` file. This file is used by TorchServe model server and defines runtime parameters for the model. The parameters under the `handler` section are used by the `inference.py` file at runtime to initialize the model.\n",
    "\n",
    "- handler\n",
    "    - `model_checkpoint_dir` - the location of the model weights folder under the model path.\n",
    "    - `amp` - runtime model parameter data type\n",
    "    - `tp_degree` - tensor parallelism degree, the number of neuron cores available. Inf2.xlarge has 1 accelerator with 2 cores\n",
    "    - `max_length` - max allowed generation length for the model\n",
    "    \n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec33c5d-37db-46ae-85ae-fd2bbc46e014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build/inference/model-config.yaml\n",
    "\n",
    "minWorkers: 1\n",
    "maxWorkers: 1\n",
    "maxBatchDelay: 10\n",
    "responseTimeout: 10800\n",
    "batchSize: 1\n",
    "\n",
    "handler:\n",
    "    model_checkpoint_dir: \"llama2-split\"\n",
    "    amp: \"f16\"\n",
    "    tp_degree: 2 \n",
    "    max_length: 4096\n",
    "\n",
    "micro_batching:\n",
    "    micro_batch_size: 1 \n",
    "    parallelism:\n",
    "        preprocess: 2\n",
    "        inference: 1\n",
    "        postprocess: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaea274-ceef-46e9-9b19-dfbbe3e32273",
   "metadata": {},
   "source": [
    "***\n",
    "Create the models `requirements.txt` file to be installed during deployment. The tokenizer requires the `sentencepiece` package.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59622e6-3f2e-47e7-9802-0eebcad8c543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build/inference/requirements.txt\n",
    "\n",
    "sentencepiece==0.1.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388e2285",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Define the `model_name` constant. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Important: </b> Please refrain from altering the value, as other parts of the workshop rely on it being set to <b>llama-2-7b</b>. </div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama-2-7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f244ef-a097-4ff2-a131-9ef6a1958300",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Run the [torch-model-archiver](https://github.com/pytorch/serve/blob/master/model-archiver/README.md) to create the files and folder layout TorchServe expects.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a2fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd build/inference && torch-model-archiver --model-name {model_name} --version 1.0 --handler inference.py -r requirements.txt --config-file model-config.yaml --archive-format no-archive --force\n",
    "\n",
    "print(\"Model package directory contents: \\n\")\n",
    "\n",
    "!tree \"./build/inference/llama-2-7b/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab791fd",
   "metadata": {},
   "source": [
    "## Update the model archive\n",
    "\n",
    "Read the model_data.json file created from the prepare model notebook to get the S3 uri of the model weights. If your workshop is providing shared model weights, you will be prompted to enter in the S3 uri location provided to you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d4306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"model_data.json\", \"r\") as file:\n",
    "    model_data = json.load(file)\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b314a68",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Copy the inference files into the model folder in S3, or, enter in the S3 path provided to you.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Note: </b> Upon executing the subsequent code cell, should an input prompt appear, be aware that the focus will automatically shift to the cell just executed. To proceed, enter the requisite value in the input box. Subsequently, it's imperative to manually select the cell immediately below the input box; failing to do so will result in the re-rendering of the input box, thereby erasing the previously entered value. </div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7dfc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy the files to S3\n",
    "model_s3_uri = model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "if model_s3_uri != \"\":\n",
    "    !aws s3 cp build/inference/{model_name}/ {model_s3_uri} --recursive\n",
    "    s3_location_input = None\n",
    "else:\n",
    "    # Use IpWidgets Text to ask for the s3 model location then update the S3Uri\n",
    "    \n",
    "    print(\"Input the S3 location of the model artifact:\")\n",
    "    print(\" * Remember, after entering the value in the text box, you will need to manually select the next cell to continue running the notebook.\")\n",
    "    s3_location_input = widgets.Text(\n",
    "        placeholder=\"Enter S3 Uri\",\n",
    "        description=\"S3:\",\n",
    "        disabled=False\n",
    "    )\n",
    "    display(s3_location_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51748c95",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Validate the S3 model path.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c4761-035d-46f9-babd-75c365e22d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If s3 location was entered, validate format and update model_data object\n",
    "if s3_location_input and s3_location_input.value != \"\":\n",
    "    s3_location = s3_location_input.value\n",
    "\n",
    "    if not s3_location.endswith(\"/\"):\n",
    "        s3_location += \"/\"\n",
    "    model_data[\"S3DataSource\"][\"S3Uri\"] = s3_location\n",
    "\n",
    "# asset s3uri is not empty\n",
    "assert (\n",
    "    model_data[\"S3DataSource\"][\"S3Uri\"] != \"\"\n",
    "), \"Model S3 source is missing. Please provide a S3 location.\"\n",
    "\n",
    "print(f\"S3 model uri {model_data['S3DataSource']['S3Uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b0d76",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "When the model weights are combined with the TorchServe files, the directory structure will look like the image below.\n",
    "\n",
    "![](./assets/images/model-package.png)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de93d4",
   "metadata": {},
   "source": [
    "## Create and deploy model endpoint\n",
    "\n",
    "\n",
    "Create the SageMaker model and deploy the endpoint. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Important: </b> Endpoint deployment takes around 10 minutes, and since we won't use the endpoint until lab 5, you don't need to wait for the cell to finish before moving on. So after running the cell below, go ahead and move on to the next lab.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe6c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = Model(\n",
    "    name=sagemaker.utils.name_from_base(model_name, short=True),\n",
    "    model_data=model_data,\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    env={\n",
    "        \"TS_INSTALL_PY_DEP_PER_MODEL\": \"true\",\n",
    "        \"FI_EFA_FORK_SAFE\": \"1\",  # https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/nrt-troubleshoot.html\n",
    "    },\n",
    "    # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "    # vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    # }\n",
    ")\n",
    "model._is_compiled_model = True\n",
    "\n",
    "model.deploy(\n",
    "    endpoint_name=model_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.inf2.xlarge\",\n",
    "    volume_size=64,\n",
    "    model_data_download_timeout=3600,\n",
    "    container_startup_health_check_timeout=600,\n",
    "    region=region,\n",
    ")\n",
    "endpoint_name = model.endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d34e5c-0be8-445d-b986-be7db5e4c115",
   "metadata": {},
   "source": [
    "Remember, after clicking run on the cell above, to move to the next lab.\n",
    "\n",
    "## Notebook complete\n",
    "\n",
    "You've deploy Llama 2 Chat on Inferenia 2. Move to the next notebook to dive deep into text embeddings and semantic search where we will learn about information retrieval for private data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
