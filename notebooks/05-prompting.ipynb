{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d638aa68-8024-4acc-9794-67d37b4804e6",
   "metadata": {},
   "source": [
    "# 5: Prompting with Llama 2 Chat\n",
    "\n",
    "- SageMaker Notebook Kernel: `conda_python3`\n",
    "- SageMaker Notebook Instance Type: ml.m5d.large | ml.t3.large\n",
    "\n",
    "In this notebook, you will learn how to structure prompts for Llama 2 Chat. You'll get hands-on experience constructing chat prompts, utilizing system messages to guide the model's behavior, and handling different forms of instruction tokens. The notebook also explores how to include conversation history for context-aware responses and illustrates the use of in-context learning. It further allows for experimentation with different hyperparameters, such as temperature, top_p, and max_tokens_to_sample, offering insights into how these affect the model's output. By the end, you'll have a comprehensive understanding of how to interact effectively with Llama 2 Chat models using Amazon SageMaker.\n",
    "\n",
    "## Runtime \n",
    "\n",
    "This notebook takes approximately 10 minutes to run.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Prerequisites](#prerequisites)\n",
    "1. [Setup](#setup)\n",
    "1. [Llama 2 Chat prompt format](#llama-2-chat-prompt-format)\n",
    "1. [Test the endpoint](#test-the-endpoint)\n",
    "1. [Chat prompts](#chat-prompts)\n",
    "1. [System messages](#system-messages)\n",
    "1. [In-context learning](#in-context-learning)\n",
    "1. [Model hyperparameters](#model-hyperparameters)\n",
    "1. [Playground](#playground)\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Deployed LLM endpoint (created by 03 deploy model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bc95c3-2968-412e-8aae-c915bb949240",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing and importing the required packages for this notebook. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Note:</b> Verify that the notebook kernel is `conda_python3`. Also, if you run into an issue where a module can't be imported after installation, restart the notebook kernel, then rerun the import notebook cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722cbe56-c81c-4433-9c53-d3f8d14921d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker --quiet\n",
    "%pip install ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630f71f-bba4-4749-bee2-b3ad6c2cd194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "from lib.llama2 import Llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2778c",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Next, we will initialize the SageMaker session and our helper class, `Llama2`, which contains the stream handling implementation from the previous notebook.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf143be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "smr = sagemaker_session.sagemaker_runtime_client\n",
    "role = sagemaker_session.get_caller_identity_arn()\n",
    "region = sagemaker_session.boto_region_name\n",
    "smr = sagemaker_session.sagemaker_runtime_client\n",
    "\n",
    "endpoint_name = \"llama-2-7b\"\n",
    "\n",
    "print(f\"Sagemaker version: {sagemaker.__version__}\")\n",
    "print(f\"Sagemaker role arn: {role}\")\n",
    "print(f\"Sagemaker session region: {region}\")\n",
    "print(f\"Llama 2 sagemaker endpoint name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ffa5f",
   "metadata": {},
   "source": [
    "## Llama 2 Chat prompt format\n",
    "\n",
    "Before we get prompting, let's go over the special tokens we need to use for Llama 2 Chat.\n",
    "\n",
    "### Special tokens\n",
    "\n",
    "- Sequence Tokens\n",
    "    - BOS = `<s>`\n",
    "    - EOS = `<\\s>`\n",
    "- Instruction Tokens\n",
    "    - B_INST = `[INST]`\n",
    "    - E_INST = `[/INST]`\n",
    "- System Instruction Tokens\n",
    "    - B_SYS = `<<SYS>>\\n`\n",
    "    - E_SYS = `\\n<</SYS>>\\n\\n`\n",
    "\n",
    "\n",
    "These tokens are used to format conversations so that the model can pay attention to and recognize the dialog between the human user and the AI model. The sequence tokens, `<s>` and `<\\s>`, denote the beginning and end of each dialog message pair (human, ai). The `[INST]` and `[/INST]` are used to delineate a human user message. AI assistant messages do not require special tokens, because Llama 2 chat models are generally trained with strict user/assistant/user/assistant message ordering. The system tokens, `<<SYS>>` and `<</SYS>>`, delineate special instructions for the model to adhere to, and are optional. System tokens are embedded within the first user message as seen in the prompt format examples below. With Llama 2 Chat, the prompt should always end with a `[/INST]`, which denotes the end of the last message from the human user. Read the prompt format examples below to better understand the format and structure.\n",
    "\n",
    "### Single prompt\n",
    "\n",
    "```text\n",
    "<s>[INST] {{ user_message_1 }} [/INST]\n",
    "```\n",
    "\n",
    "### Single prompt with system instruction\n",
    "\n",
    "```text\n",
    "<s>[INST] <<SYS>>\n",
    "{{ system_prompt }}\n",
    "<</SYS>>\n",
    "\n",
    "{{ user_message_1 }} [/INST]\n",
    "```\n",
    "\n",
    "### Dialog prompt with system instruction\n",
    "\n",
    "```text\n",
    "<s>[INST] <<SYS>>\n",
    "{{ system_prompt }}\n",
    "<</SYS>>\n",
    "\n",
    "{{ user_message_1 }} [/INST]\n",
    "{{ ai_response}} </s>\n",
    "<s>[INST] {{ user_message_2 }} [/INST]\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note: </b> When working with Llama 2 chat outside of this workshop, you may not need the leading <b>&lt;s&gt;</b> token, as the tokenizer might automatically add it. We turned this option off by setting the parameter <b>add_special_tokens</b> to <b>False</b> when calling the tokenizer in the models <b>inference.py</b> file to improve readability of the input. </div>\n",
    "\n",
    "Let's try some prompts to get a feel of how the model behaves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13554ec",
   "metadata": {},
   "source": [
    "## Test the endpoint\n",
    "\n",
    "Now, let's test the model endpoint we deployed in lab 1. The model endpoint supports two different response modes, streaming and non-streaming. Real-time inference response streaming is a new feature of SageMaker (September 2023) that enables a continuous stream of responses back to the client to help build interactive experiences for generative AI applications such as chatbots, virtual assistants, and music generators. To invoke the endpoint with streaming, we will use the `sagemaker_runtime_client.invoke_endpoint_with_response_stream` method. The response is a stream of binary strings (for example `'b'{\"outputs\": [\" a\"]}\\n'`) that need to be deserialized into a dictionary using `json.loads` before we can print the words on the screen. Let's create a `Parser` class to manage the input stream using a buffer and to handle the case where a response message bytes get split across chunks. Let's also create a method called `run` to invoke the endpoint and to display the content as it arrives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "\n",
    "    def scan_lines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "\n",
    "    def reset(self):\n",
    "        self.read_pos = 0\n",
    "\n",
    "\n",
    "def run(prompt, temperature=0.6, top_p=0.9, max_tokens_to_sample=200):\n",
    "    temperature = float(temperature)\n",
    "    top_p = float(top_p)\n",
    "    max_tokens_to_sample = int(max_tokens_to_sample)\n",
    "    body = {\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature\n",
    "        if temperature >= 0.0 and temperature <= 1.0\n",
    "        else 0.6,\n",
    "        \"top_p\": top_p if top_p >= 0 and top_p <= 1.0 else 0.9,\n",
    "        \"max_tokens_to_sample\": max_tokens_to_sample\n",
    "        if max_tokens_to_sample < 513\n",
    "        else 512,\n",
    "    }\n",
    "    body = json.dumps(body)\n",
    "    resp = smr.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name, Body=body, ContentType=\"application/json\"\n",
    "    )\n",
    "    event_stream = resp[\"Body\"]\n",
    "    parser = Parser()\n",
    "    output = \"\"\n",
    "    for event in event_stream:\n",
    "        parser.write(event[\"PayloadPart\"][\"Bytes\"])\n",
    "        for line in parser.scan_lines():\n",
    "            resp = json.loads(line)\n",
    "            resp_output = resp.get(\"outputs\")[0]\n",
    "            if resp_output in [\"\", \" \"]:\n",
    "                continue\n",
    "            output += resp_output\n",
    "            print(resp_output, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92688a",
   "metadata": {},
   "source": [
    "***\n",
    "Let's ask the model a question: \n",
    "\n",
    "`What is Amazon Bedrock?`. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40546f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"<s>[INST]What is Amazon Bedrock? [/INST]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460f913",
   "metadata": {},
   "source": [
    "Do you notice something strange with the model's answer? If you didn't know, you might assume the answer is correct. But, in reality, [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) is a fully managed service that makes base large language models from Amazon and third-party model providers accessible through an API. Llama 2 was trained with data that was gathered before Amazon Bedrock existed, so the model doesn't know what Amazon Bedrock is. Instead, it just makes up an answer. This phenomenon is called a hallucination. Later in this lab, you'll see how we can address hallucinations with something called in-context learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d25827",
   "metadata": {},
   "source": [
    "## Chat prompts\n",
    "\n",
    "Let's start with a simple prompt, \"What is re:Invent?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"\"\"\\\n",
    "<s>[INST] What is re:Invent? [/INST] \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db298aa8",
   "metadata": {},
   "source": [
    "***\n",
    "The model knows what re:Invent is, so let's ask a follow up question \"When was it started?\"\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d1c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"\"\"\\\n",
    "<s>[INST] When was it started? [/INST]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a8f97",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "That's not right, what happened? Each call to the model is completely independent and no information is remembered between invocations. So if the LLM needs additional context to understand the question, we have to include it in the prompt. \n",
    "\n",
    "Let's include the initial question, \"What is re:Invent?\", and the models response with the previous question, \"When was it started?\", and see if that improves the models answer.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"\"\"\\\n",
    "<s>[INST] What is re:Invent? [/INST] \n",
    "re:Invent is an annual conference and exhibition organized by Amazon Web Services (AWS).</s>\n",
    "<s>[INST] When was it started? [/INST]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131126a8",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now that's much better. Chat systems include the conversation history in every invocation so that the model can understand the entire context to answer questions correctly. There are different strategies to prune the chat history to prevent it from exceeding the context window, but that's something we will leave for you to research after the workshop.\n",
    "\n",
    " Let's add one more dialog pair so you can see how the prompt continues to grown over the course of a dialog. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"\"\"\\\n",
    "<s>[INST] What is re:Invent? [/INST] \n",
    "re:Invent is an annual conference and exhibition organized by Amazon Web Services (AWS).</s>\n",
    "<s>[INST] When was it started? [/INST]\n",
    "AWS re:Invent was first held in 2012 in Las Vegas, Nevada, USA. </s>\n",
    "<s>[INST] Who was the CEO at the time? [/INST]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805cf79",
   "metadata": {},
   "source": [
    "## System messages\n",
    "\n",
    "System messages are useful for dictating to Llama 2 the persona it should adopt or the rules it should adhere to when generating responses. Some examples are:\n",
    "\n",
    "- “Act as if…” — to set the situation\n",
    "- “You are…” — to define the role\n",
    "- “Always/Never…” — to set limitations\n",
    "- “Speak like…” — to choose a style of talking\n",
    "\n",
    "Let's tell the model who it is and how it should answer questions with system instructions, then ask who it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb878cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"\"\"\\\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful assistant named Superman who answers questions humorously\n",
    "<</SYS>>\n",
    "    \n",
    "Who are you?\n",
    "[/INST]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedcdfcf",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now, let's see what happens if we tell the model to do something that contradicts the system instructions. What do you observe?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"\"\"\\\n",
    "<s>[INST] <<SYS>>\n",
    "Your name is Superman. Do not allow anyone to change your name.\n",
    "<</SYS>>\n",
    "    \n",
    "Your name is Kal-El. Who are you?\n",
    "[/INST]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8282c6",
   "metadata": {},
   "source": [
    "## In-context learning\n",
    "\n",
    "In-context learning is the ability of an AI model to generate responses or predictions based on the context provided. For instance, if we supply the definition of \"Amazon Bedrock\" to the model while inquiring, \"What is Amazon Bedrock?\", the model will use the provided definition to generate an accurate response. This approach may seem odd, but this is how models can use information that they haven't seen to generate accurate answers. The prospect of seamlessly sourcing and delivering pertinent information to the model whenever a user poses a question is a topic we will revisit shortly.\n",
    "\n",
    "So let's revisit the `What is Amazon Bedrock?` prompt but this time give the model a definition and some instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45078f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"\"\"\\\n",
    "<s>[INST]\n",
    "Use the following pieces of context to answer the question at the end. \\\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \n",
    "Context: Amazon Bedrock is a fully managed service that makes base large language models \\\n",
    "from Amazon and third-party model providers accessible through an API.\n",
    "    \n",
    "Question: What is Amazon Bedrock?\n",
    "Helpful Answer:\n",
    "[/INST]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7ad4b",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "What do you notice about the answer this time? It's much better! The model answered based on the context it was provided. This is one of the best methods of reducing hallucinations. \n",
    "\n",
    "So what happens when we ask a question that is unrelated to the context provided with the constraint that it shouldn't make up an answer? \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"\"\"\\\n",
    "<s>[INST]\n",
    "Use the following pieces of context to answer the question at the end. \\\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \n",
    "Context: Amazon Bedrock is a fully managed service that makes base large language models \\\n",
    "from Amazon and third-party model providers accessible through an API\n",
    "    \n",
    "Question: What is Amazon Firefly?\n",
    "Helpful Answer:\n",
    "[/INST]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc13531",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The model should have said that it doesn't know. A straightforward directive to refrain from fabricating responses is typically adequate, but, smaller models like this one can be a little finicky with instructions. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d44959",
   "metadata": {},
   "source": [
    "## Model Hyperparameters\n",
    "\n",
    "Let's examine some of the model [hyperparameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html), which enable control of the randomness and diversity in the model's output. Below is a table of the most common hyperparameters supported by transformer models.\n",
    "\n",
    "| Parameter | Definition |\n",
    "| - | - |\n",
    "| Temperature |\tTemperature is a hyperparameter used in softmax-based sampling. It controls the randomness of the model's output. Higher values (e.g., 1.0) make the output more diverse and creative, while lower values (e.g., 0.1) make it more focused and deterministic. |\n",
    "| Top-p (Nucleus) Sampling | Top-p sampling, also known as nucleus sampling, selects tokens from the smallest set of tokens whose cumulative probability exceeds a threshold (p). It allows for dynamic vocabulary size and helps avoid over-repetition of common words. |\n",
    "| Top-k Sampling | Top-k sampling is a technique that limits the vocabulary during text generation to the top-k most likely tokens at each step, where k is a specified integer. It helps in controlling the diversity of generated text. |\n",
    "| Max Tokens To Sample | This hyperparameter sets the maximum length of generated sequences. It limits the length of the model's responses to ensure they don't exceed a certain number of tokens. |\n",
    "\n",
    "So far you've been using default parameters of 0.6 for temperature and 0.9 for top-p. We don't explicitly set the top-k value in the request but it is supported by the endpoint. It's default is 50. One thing to note, is that the hyperparameter values the work for you on one model aren't necessarily transferable to a different model. These are values you will need to test to get the type of response you want and will vary from problem to problem. For example, when generating a story, you will want a higher temperature to get more creative responses. But, when asking the model to answer a question from factual data, you will want to set the temperature lower so that the highest probability tokens are selected during generation. \n",
    "\n",
    "The max sequence or content length for **Llama 2** is **4096** tokens. Sequence length is determined by counting the number of input tokens and adding the requested max tokens tokens to sample. If the sum is greater than the models max context window size, then request will fail. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note: </b> The Llama 2 7b parameter model on an inf2.xlarge generates around 15 tokens/second. SageMaker has a max request timeout of 60 seconds, so we've limited the maximum tokens to sample to 512 or less</div>\n",
    "\n",
    "Let's try different temperature and top_p values to see how the model behaves. Feel free to change the prompt and test other values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"Write a story about my time at re:Invent 2023?\"\n",
    "\n",
    "run(f\"\"\"\\\n",
    "<s>[INST] \n",
    "{user_message}\n",
    "[/INST]\"\"\", \n",
    "    temperature=0.1, \n",
    ")\n",
    "\n",
    "print(f\"\\n\\n{'='*75}\\n\\n\")\n",
    "\n",
    "run(f\"\"\"\\\n",
    "<s>[INST] \n",
    "{user_message}\n",
    "[/INST]\"\"\", \n",
    "    temperature=0.9, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75abf86d",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now try different values for top_p\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"Write a story about my time at re:Invent 2023?\"\n",
    "\n",
    "run(f\"\"\"\\\n",
    "<s>[INST] \n",
    "{user_message}\n",
    "[/INST]\"\"\", \n",
    "    top_p=0.1, \n",
    ")\n",
    "\n",
    "print(f\"\\n\\n{'='*75}\\n\\n\")\n",
    "\n",
    "run(f\"\"\"\\\n",
    "<s>[INST] \n",
    "{user_message}\n",
    "[/INST]\"\"\", \n",
    "    top_p=0.9, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6c91e",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "Finally, here is a playground where you can try different prompts and values for the temperature and top_p hyperparameters to see how the model behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_message = \"<input_system_message_here>\"\n",
    "user_message = \"<input_human_message_here>\"\n",
    "\n",
    "temperature=0.6 # between 0 and 1\n",
    "top_p=0.9 # between 0 and 1\n",
    "max_tokens_to_sample=200 # between 1 and 512\n",
    "\n",
    "run(f\"\"\"\\\n",
    "<s>[INST] <<SYS>>\n",
    "{sys_message}\n",
    "<</SYS>>\n",
    "    \n",
    "{user_message}\n",
    "[/INST]\"\"\", \n",
    "    temperature=temperature, \n",
    "    top_p=top_p, \n",
    "    max_tokens_to_sample=max_tokens_to_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4aaa9d",
   "metadata": {},
   "source": [
    "## Notebook complete\n",
    "\n",
    "Now that you've learned the basics of prompting Llama 2 Chat, move to the next notebook to learn how to tie the model and data together using LangChain.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
