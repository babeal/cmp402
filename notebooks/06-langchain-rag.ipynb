{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Retrieval Augmented Generation Chatbot with LangChain\n",
    "\n",
    "- SageMaker Notebook Kernel: `conda_python3`\n",
    "- SageMaker Notebook Instance Type: ml.m5d.large | ml.t3.large\n",
    "\n",
    "In this notebook, we will bring together all of the pieces to form the foundation of a retrieval augmented generation chatbot using [Amazon Bedrock](https://aws.amazon.com/bedrock/), [Amazon OpenSearch](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vector-search.html) Vector Search, and [LangChain](https://python.langchain.com/). \n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. The library provides LLM model adapters, data retrieval components, text splitters, conversation memory and storage, as well as components that wire all of these things together. It also provides agents which orchestrate LLMs and tools that go beyond chat interfaces.\n",
    "\n",
    "You'll learn how to construct a simple chain with just a prompt and a model using LangChain expression language. Then you'll learn how to add retrieval components, chat history, and question rephrasing to the chain to build a chatbot that can answer questions with private data. \n",
    "\n",
    "## Runtime \n",
    "\n",
    "This notebook takes approximately 15 minutes to run.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Prerequisites](#prerequisites)\n",
    "1. [Setup](#setup)\n",
    "1. [LangChain Expression Language](#langchain-expression-language-lcel)\n",
    "1. [Simple chain](#simple-chain)\n",
    "1. [Retrieval chain](#retrieval-chain)\n",
    "1. [Chat chain](#chat-chain)\n",
    "1. [Question rephrasing](#question-rephrasing)\n",
    "1. [Retriever chain](#retriever-chain)\n",
    "1. [Chat with retriever chain](#chat-with-retriever-chain)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Deployed Llama2 inference endpoint on Amazon SageMaker\n",
    "- Bedrock user guide documentation ingested into Amazon OpenSearch Serverless Vector Store\n",
    "- `amazon.titan-embed-text-v1` embeddings model enabled in the Amazon Bedrock console in `us-west-2`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing and importing the required packages for this notebook. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Note:</b> Verify that the notebook kernel is `conda_python3`. Also, if you run into an issue where a module can't be imported after installation, restart the notebook kernel, then rerun the import notebook cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain==0.0.317 --quiet\n",
    "%pip install opensearch-py==2.3.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import langchain.vectorstores.opensearch_vector_search as ovs\n",
    "\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, helpers\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from operator import itemgetter\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import Runnable, RunnablePassthrough, RunnableLambda, RunnableMap, RunnableBranch\n",
    "from langchain.schema.messages import AIMessage, HumanMessage, SystemMessage, ChatMessage\n",
    "\n",
    "# load langchain helper classes from our app\n",
    "sys.path.append(\"../app/lib/langchain/\")\n",
    "from opensearch import create_ovs_client\n",
    "from llama2 import Llama2Chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Next, we will initialize the Amazon Bedrock boto3 client.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3 session\n",
    "boto3_session = boto3.Session()\n",
    "region = boto3_session.region_name\n",
    "\n",
    "print(f\"Boto3 region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Let's retrieve the Amazon OpenSearch collection id\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_client = boto3_session.client(\"opensearchserverless\")\n",
    "list_collections_response = aoss_client.list_collections()\n",
    "collection_id = list_collections_response.get(\"collectionSummaries\")[0].get(\"id\")\n",
    "index_name = \"bedrock-docs\"\n",
    "print(f\"OpenSearch collection name: {collection_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Finally, initialize the Embeddings, OpenSearch and LLM LangChain foundational components\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "bedrock_client = boto3_session.client(\"bedrock-runtime\")\n",
    "embeddings_model_id = \"amazon.titan-embed-text-v1\"\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_client, model_id=embeddings_model_id\n",
    ")\n",
    "\n",
    "# VectorStore (using a helper function to create LangChain OpenSearchVectorSearch with client patch)\n",
    "vector_store = create_ovs_client(\n",
    "    collection_id, index_name, region, boto3_session, bedrock_embeddings\n",
    ")\n",
    "\n",
    "# Our custom Llama2Chat class (see the /app/lib/langchain/llama2.py file)\n",
    "llm = Llama2Chat(\n",
    "    endpoint_name=\"llama-2-7b\",\n",
    "    client=boto3_session.client(\"sagemaker-runtime\"),\n",
    "    streaming=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "A retriever is a lightweight wrapper around a vector store object to make it confirm to the retriever interface use by the LangChain components. We will initialize it here with the parameter, `k`, which represents the number of documents we want returned from the vector store when a search is run. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_to_return = 2\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": num_docs_to_return})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)\n",
    "\n",
    "LangChain Expression Language (LCEL) is a declarative language designed to facilitate the composition of chains. The principle behind LCEL is to enable an easier and more intuitive interaction with core components, aiding in the construction and management of chains in a simplified manner. LCEL provides a straightforward way to compose chains, aiding in the encapsulation of different operations within a unified syntax. This composition is enhanced through intuitive pipe operations, making the engagement with core components more effortlessâ€‹.\n",
    "\n",
    "\n",
    "If you want to dive deeper into LCEL see the following link [LCEL](https://python.langchain.com/docs/expression_language/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple chain\n",
    "\n",
    "Let's start with a simple chain that invokes our LLM with a single prompt. Remember, prompting Llama2 requires the prompt to be formatted with special tokens to get good output from the model. The [Llama2Chat](../app/lib/langchain/llama2.py) class converts LangChain's role based ChatMessages (Human, AI, System) into the LLama 2 format before invoking the model, so we need to use the chat prompt template and chat message components.\n",
    "\n",
    "Let's take a look at the code that creates the chain:\n",
    "\n",
    "```python\n",
    "simple_chain = simple_prompt | llm | StrOutputParser()\n",
    "```\n",
    "\n",
    "The code may look foreign, however here's a way to think about what's happening. \n",
    "\n",
    "When invoking the chain, the input is piped to the simple prompt template for formatting, then the formatted prompt is piped into the LLM to generate a response, then the LLM's response is piped into the string output parser and the parsed output is returned to the caller. \n",
    "\n",
    "The components are considered [runnables](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.base.Runnable.html#langchain.schema.runnable.base.Runnable). A runnable is a unit of work that can be invoked, batched, streamed, transformed and composed. \n",
    "\n",
    "If you look at the `SIMPLE_TEMPLATE` you will see a placeholder `{question}`. When we call the invoke method of the chain we pass in an object with the value of that placeholder. The value gets replaced during the formatting step. Later in this notebook we will show more complex examples of these placeholders and how their values are provided from other chains.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note: </b> LangChain supports response streaming, but for the sake of simplicity, we'll be utilizing the non-streaming method in this notebook. In the next module, you will have the option to use either method.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_TEMPLATE = \"{question}\"\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_template(SIMPLE_TEMPLATE)\n",
    "\n",
    "simple_chain = (\n",
    "    simple_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "simple_chain.invoke({\"question\": \"What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval chain\n",
    "\n",
    "Let's add the vector store retriever to create a retrieval augmented generation (RAG) chain. Our prompt template now has two placeholders; One for the context retrieved by the vector store and one for the question. \n",
    "\n",
    "Our input to the invoke method is the question that needs to be used with the retriever to get the context and both the question and context need to be passed to the chat prompt (`rag_prompt`). We do this by creating a dictionary at the beginning of the chain with properties that are runnables. `itemgetter` picks values out of the input and allows us to pipe the question to the retriever and then bind that value to the `context`` key of the dictionary before it's passed into the prompt formatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\\\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke({\"question\": \"What is Amazon Bedrock?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat chain \n",
    "\n",
    "So far we have only looked at single prompts, but chats are dialogs with many question and answer pairs. For conversational interfaces we need to include the dialog between the human and the ai so that the LLM can understand the entire context when responding to the question. \n",
    "\n",
    "We will update our prompt template to include a placeholder for `chat_history` and then pass the history when invoking the chain. LangChain uses message types to represent the dialog entities: `SystemMessage`, `HumanMessage`, and `AIMessage`. Remember that for Llama 2 the model expects a specific format for these messages which is built into the `Llama2Chat` class. We just need to make sure that the chat history we pass always has alternating human and ai messages with an optional system message at the begging otherwise you will get an exception.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = (\n",
    "    chat_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"What is Amazon Bedrock?\"),\n",
    "    AIMessage(content=\"Amazon Bedrock is a fully managed service that makes base models from Amazon and third-party model providers accessible through an API.\"),\n",
    "]\n",
    "\n",
    "chat_chain.invoke({\"question\": \"What does fully managed mean?\",  \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question rephrasing \n",
    "\n",
    "Before we integrate the chat and retrieval systems, a challenge arises in comprehending user messages without the context of the preceding conversation. Consider a scenario where a user inquires, `What's Amazon Bedrock?`, followed by, `What models does it support?`. To effectively retrieve relevant documents from the vector store, we need to have an input encapsulating all pertinent contextual information from the dialogue or we won't retrieve the right data. \n",
    "\n",
    "We can accomplish this with a question rephrasing step that uses the LLM, the chat history, and the users question to generate a stand alone rephrased statement that can be used by the vector store to retrieve documents. \n",
    "\n",
    "Run the cell below to see how the users question `What models does it support?` is rephrased into a standalone question that can be passed to the vector store for search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPHRASE_TEMPLATE = \"\"\"\\\n",
    "Given the following conversation and a follow up question, rephrase the follow up \\\n",
    "question to be a standalone question. Only return the standalone question and nothing else.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone Question:\\\n",
    "\"\"\"\n",
    "\n",
    "condense_question_chain = (\n",
    "    PromptTemplate.from_template(REPHRASE_TEMPLATE)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "condense_question_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"question\": \"What models does it support?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Let's combine the `condense_question_chain` and the `retriever` and see what the response looks like. Does the content of the returned documents seem correct? \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_condense_chain = condense_question_chain | retriever\n",
    "\n",
    "retriever_condense_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"question\": \"What models does it support?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever chain\n",
    "\n",
    "There is a case when we don't want to run the condense question chain and that's when there is no history. So we will use the `RunnableBranch` helper that only executes `retriever_condense_chain` if there is a `chat_history`, otherwise it just passes the question directly to the `retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chain = RunnableBranch(\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))),\n",
    "        retriever_condense_chain,\n",
    "    ),\n",
    "    (RunnableLambda(itemgetter(\"question\")) | retriever)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Let's run the retriever chain without chat history and see what happens. Notice that the standalone question prompt isn't executed and the question is passed through to the vector store.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chain.invoke({\n",
    "    \"chat_history\": None, \n",
    "    \"question\": \"What models does it support?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now run it with history to verify that the retriever chain is working correctly.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history, \n",
    "    \"question\": \"What models does it support?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with retriever chain\n",
    "\n",
    "Let's combine all of the pieces together to create a retrieval augmented chat chain. First, let's define the system prompt template which will contain instructions for the model to follow with a place holder for the content retrieved by the retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_TEMPLATE = \"\"\"\\\n",
    "Generate a comprehensive and informative answer of 80 words or less for the \\\n",
    "given question based solely on the provided search results (URL and content). You must \\\n",
    "only use information from the provided search results. Use an unbiased and \\\n",
    "journalistic tone. Combine search results together into a coherent answer. Do not \\\n",
    "repeat text. Cite search results using [${{number}}] notation. Only cite the most \\\n",
    "relevant results that answer the question accurately. Place these citations at the end \\\n",
    "of the sentence or paragraph that reference them - do not put them all at the end. If \\\n",
    "different results refer to different entities within the same name, write separate \\\n",
    "answers for each entity.\n",
    "\n",
    "You should use bullet points in your answer for readability. Put citations where they apply\n",
    "rather than putting them all at the end.\n",
    "\n",
    "If there is nothing in the context relevant to the question at hand, just say \"Hmm, \\\n",
    "I'm not sure.\" Don't try to make up an answer.\n",
    "\n",
    "Anything between the following <context></context>  html blocks is retrieved from a knowledge \\\n",
    "bank, not part of the conversation with the user. \n",
    "\n",
    "<context>\n",
    "    {context} \n",
    "</context>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The content returned by the retriever is a list of `Documents` and we want to add some formatting for the LLM to understand that they are separate. `format_docs` is a helper method that takes a list of documents and formats each document text within `<doc></doc>` tags. The LLM can use this information to cite specific sources when providing responses.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_string = f\"<doc id='{i}'>{doc.page_content}</doc>\"\n",
    "        formatted_docs.append(doc_string)\n",
    "    return \"\\n\".join(formatted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Previously, we use the `HumanMessage` and `AIMessage` classes, but let's create a helper method that takes a list of simple messages and converts into the correct types. This will be useful later when we build the chat application as in a production application you would need to serialize and deserialize the chat history between invocations from the user. It also makes it easier for us to play with the inputs.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_history(request):\n",
    "    chat_history = request[\"chat_history\"] or []\n",
    "    converted_chat_history = []\n",
    "    for message in chat_history:\n",
    "        if message.get(\"human\") is not None:\n",
    "            converted_chat_history.append(HumanMessage(content=message[\"human\"]))\n",
    "        if message.get(\"ai\") is not None:\n",
    "            converted_chat_history.append(AIMessage(content=message[\"ai\"]))\n",
    "    return converted_chat_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now we will complete the chain by combining the previous parts. The following list explains each part in the code below.  \n",
    "\n",
    "- `input_map` - maps over the input and converts the simple chat_history into message type objects.\n",
    "- `context_chain` - passes the `input_map` to the retriever chain to rephrase question and search for content, then formats the content and maps it to the context variable. The question and chat_history are passed through.\n",
    "- `rag_prompt` - combines the context, chat_history, and question with the system prompt to be executed by the llm\n",
    "- `chat_retrieval_chain` - combines the workflow steps into a chain. `input -> input_map `\n",
    "\n",
    "![](./assets/images/langchain-chat-retrieval-flow.png)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_map = {\n",
    "        \"question\": RunnableLambda(itemgetter(\"question\")),\n",
    "        \"chat_history\": RunnableLambda(serialize_history),\n",
    "    }\n",
    "\n",
    "context_chain = RunnableMap(\n",
    "    {\n",
    "        \"context\": retriever_chain | format_docs,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYSTEM_TEMPLATE),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_retrieval_chain = (\n",
    "    input_map\n",
    "    | context_chain\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Finally, let's run the chain by providing some chat history and a question.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_retrieval_chain.invoke({\n",
    "    \"chat_history\": [\n",
    "        {\"human\": \"What is Amazon Bedrock?\"},\n",
    "        {\"ai\": \"Amazon Bedrock is a fully managed service that makes base models from Amazon and third-party model providers accessible through an API.\"},\n",
    "    ],\n",
    "    \"question\": \"What models does it support?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook complete\n",
    "\n",
    "So far, you've learned about all of the foundational elements to build a retrieval augmented generation chatbot that can answer questions with private data. Next, head back to the workshop content to learn how to incorporate these components into a Streamlit app to build and end-to-end solution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
